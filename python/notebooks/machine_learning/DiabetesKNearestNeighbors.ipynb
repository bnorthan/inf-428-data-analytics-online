{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Pandas and Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the libraries we will need.  Note we have a new friend, sklearn a.k.a. [scikit-learn](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import data with pandas, following the ussual steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 9)\n",
      "Index(['Pregnancy', 'Glucose', 'bp', 'skinfold', 'insulin', 'bmi', 'dpf',\n",
      "       'age', 'class'],\n",
      "      dtype='object')\n",
      "There are 9 collumns\n",
      "Since one is a class there are 8 features\n",
      "There are 393 rows.  Each row is a feature vector\n"
     ]
    }
   ],
   "source": [
    "diabetes=pd.read_csv('indians-diabetes.csv')\n",
    "\n",
    "print (diabetes.shape)\n",
    "print (diabetes.columns)\n",
    "diabetes.head()\n",
    "\n",
    "print ('There are',diabetes.shape[1], 'collumns')\n",
    "print ('Since one is a class there are',diabetes.shape[1]-1,'features')\n",
    "print ('There are',diabetes.shape[0],'rows.  Each row is a feature vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with KNIME we have to define features and class.  In KNIME we do this through the GUI.  In Python we do this through code.  So we need to assign the feature set and class labels to variables.  In this eamples we call the features 'x' and the class 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of x\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "x\n",
      "   Pregnancy  Glucose  bp  skinfold  insulin   bmi    dpf  age\n",
      "0          1       89  66        23       94  28.1  0.167   21\n",
      "1          0      137  40        35      168  43.1  2.288   33\n",
      "2          3       78  50        32       88  31.0  0.248   26\n",
      "3          2      197  70        45      543  30.5  0.158   53\n",
      "4          1      189  60        23      846  30.1  0.398   59\n"
     ]
    }
   ],
   "source": [
    "# got the x (feature table) by removing the class from the original data\n",
    "# (the rest of the data will be features)\n",
    "x=diabetes.drop('class',axis=1)\n",
    "\n",
    "print (\"type of x\")\n",
    "# note x is a numpy array\n",
    "print (type(x))\n",
    "\n",
    "print ('x')\n",
    "print (x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the class values and assign them to a variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: class, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# get the y (the class) by just retrieving the class from the original data\n",
    "y=diabetes['class']\n",
    "\n",
    "print ('y')\n",
    "print (y.head())\n",
    "\n",
    "print (type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert x (the feature vectors) and y (the class labels) to numpy array.  That means we are converting them to numbers so we can do machine learning calculations and training with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get x and y as numeric (numpy) arrays\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (\"get x and y as numeric (numpy) arrays\")\n",
    "x=x.values\n",
    "y=y.values\n",
    "\n",
    "print(type(x))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide into training and testing sets.  This is equivalent to the KNIME partitioning step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=200\n",
    "\n",
    "x_train=x[:train_size,:]\n",
    "y_train=y[:train_size]\n",
    "x_test=x[train_size:,:]\n",
    "y_test=y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a nearest neighbors learner.  This step is equivalent to setting up the KNIME K nearest neighbor node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "nn=neighbors.KNeighborsClassifier(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the traininng data.  For k-nearest neighbors this basically means 'set up' the machine learning system, in other systems (such as decision trees, and neural networks) there would be a more involved training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier on the testing data.  The result is a vector of 'predicitons'.  0 indicates the first class (no diabetes) and 1 is the second class (diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193,)\n",
      "[0 1 0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "predictions_nn=nn.predict(x_test)\n",
    "type(predictions_nn)\n",
    "print(predictions_nn.shape)\n",
    "print(predictions_nn[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we score the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neareset Neighbors\n",
      "train size:  200 has accuracy: 0.7253886010362695\n"
     ]
    }
   ],
   "source": [
    "nn_score=accuracy_score(y_test, predictions_nn)\n",
    "\n",
    "print (\"Neareset Neighbors\")\n",
    "print (\"train size: \",train_size,\"has accuracy:\", nn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try Naive Bayes.  Naive Baye's is a machine learning method based on Baye's rule and conditional probability.  Meaning we can take into account prior knowledge, specifically the probality of the class occurring.  Note the code is almost exactly the same as for K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier on the testing data and compute the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes\n",
      "train size:  200 has accuracy: 0.7772020725388601\n"
     ]
    }
   ],
   "source": [
    "predictions_gnb=gnb.predict(x_test)\n",
    "gnb_score=accuracy_score(y_test, predictions_gnb)\n",
    "\n",
    "print (\"Gaussian Naive Bayes\")\n",
    "print (\"train size: \",train_size,\"has accuracy:\", gnb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try Logistic Regression.  Logistic Regression is a machine learning technique that uses optimizaation to find the best regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the training data (this is equivalent to the learning stage in Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnorthan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now make the predictions and calculate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic\n",
      "train size:  200 has accuracy: 0.7927461139896373\n"
     ]
    }
   ],
   "source": [
    "predictions_log=logistic.predict(x_test)\n",
    "logistic_score=accuracy_score(y_test, predictions_log)\n",
    "    \n",
    "print (\"logistic\")\n",
    "print (\"train size: \",train_size,\"has accuracy:\", logistic_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
